{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFj1OQmN3UCml+JjQvmZuq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhesh1594/siddhesh1594.github.io/blob/main/linesearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VnHQj5K2fIR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "class rosenbrock:\n",
        "    def __init__(self,a=1.0,b=100.0):\n",
        "        self.a: float = a\n",
        "        self.b: float = b\n",
        "\n",
        "    def objective(self,x: NDArray) -> float:\n",
        "        x1, x2 = x[0], x[1]\n",
        "        arr = (self.a - x1)**2 + self.b*(x2 - x1**2)**2\n",
        "        return float(arr) # chatgpt says it should return a scalar value and not np.array([arr])\n",
        "        # but with scalar value the loop got stuck\n",
        "    def grad_objective(self,x: NDArray) -> NDArray:\n",
        "        x1, x2 = x[0], x[1]\n",
        "        dfdx1 = -2*(self.a - x1) - 4*self.b*x1*(x2 - x1**2)\n",
        "        dfdx2 =  2*self.b*(x2 - x1**2)\n",
        "        return np.array([dfdx1, dfdx2])\n",
        "\n",
        "def steepest_descent(\n",
        "        x:NDArray,\n",
        "        problem:rosenbrock,\n",
        "        tol:float = 1e-6,\n",
        "        iteration_limit:int = 100000\n",
        "        ):\n",
        "    iterations = 0\n",
        "    print(\"\\n---Steepest Descen Logs---\")\n",
        "    'rosenbrock.grad_objective() missing 1 required positional argument: x'\n",
        "    '# call object and use the function # error message'\n",
        "\n",
        "    while iterations < iteration_limit:\n",
        "        'function = rosenbrock() # create object # no need of this line as problem is already calling rosenborck'\n",
        "\n",
        "        #compute gradient at current x\n",
        "        grad = problem.grad_objective(x)\n",
        "\n",
        "        #search direction\n",
        "        direction = -grad\n",
        "\n",
        "        grad_norm = np.linalg.norm(direction,ord=6)\n",
        "\n",
        "        #log iteration and gradient norm\n",
        "        #print(f\"Iter {iterations:4d} | x = {x} | f(x) = {problem.objective(x):.6e} | ||grad|| = {grad_norm:.6e}\")\n",
        "        #convergence check\n",
        "        if grad_norm <= tol:\n",
        "            print(\"\\nConverged Successfully.\")\n",
        "            return x,iterations,problem.objective(x)\n",
        "        # include line search inside steepest descent\n",
        "        alpha = linesearch(base_x=x,direction=direction)\n",
        "        #update x\n",
        "        x = x + alpha * direction\n",
        "        iterations += 1\n",
        "\n",
        "    print(\"\\nreached iteration limit without convergence\")\n",
        "    return x,iterations,problem.objective(x)\n",
        "\n",
        "def linesearch(\n",
        "    base_x: NDArray,\n",
        "    direction: NDArray,\n",
        "    alpha_min: float = 1e-6,\n",
        "    alpha_max: float = 1.0,\n",
        "    alpha_ini: float = 0.5e-2,\n",
        "    m1: float = 0.01,\n",
        "    m2: float = 0.90,\n",
        "    max_iter: int = 100,\n",
        ") -> float:\n",
        "    fn = rosenbrock()\n",
        "\n",
        "    # Precompute at base point\n",
        "    f0 = fn.objective(base_x)          # scalar\n",
        "    g0 = fn.grad_objective(base_x)     # (n,)\n",
        "    slope0 = g0 @ direction            # scalar, should be < 0 for descent\n",
        "\n",
        "    if slope0 >= 0:\n",
        "        raise ValueError(\"Direction is not a descent direction!\")\n",
        "\n",
        "    alpha_L = alpha_min\n",
        "    alpha_R = alpha_max\n",
        "    alpha = alpha_ini\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        x_trial = base_x + alpha * direction\n",
        "        phi = fn.objective(x_trial)\n",
        "        g_trial = fn.grad_objective(x_trial)\n",
        "        phi_prime = g_trial @ direction\n",
        "\n",
        "        armijo_rhs = f0 + m1 * alpha * slope0\n",
        "\n",
        "        armijo_ok = (phi <= armijo_rhs)\n",
        "        curvature_ok = (abs(phi_prime) <= m2 * abs(slope0))  # Strong Wolfe\n",
        "\n",
        "        # print(\n",
        "        #     f\"it={k:2d}, alpha={alpha:.3e}, phi={phi:.3e}, \"\n",
        "        #     f\"phi'={phi_prime:.3e}, Armijo={armijo_ok}, Curv={curvature_ok}\"\n",
        "        # )\n",
        "\n",
        "        # If both conditions satisfied → done\n",
        "        if armijo_ok and curvature_ok:\n",
        "            return float(alpha)\n",
        "\n",
        "        # If Armijo fails → step too big → move right bound\n",
        "        if not armijo_ok:\n",
        "            alpha_R = alpha\n",
        "\n",
        "        # If curvature fails but Armijo OK → step too small → move left bound\n",
        "        elif not curvature_ok:\n",
        "            alpha_L = alpha\n",
        "\n",
        "        # New alpha by bisection\n",
        "        alpha = 0.5 * (alpha_L + alpha_R)\n",
        "\n",
        "        # Safety: if interval too small\n",
        "        if abs(alpha_R - alpha_L) < 1e-12:\n",
        "            print(\"Interval too small, stopping.\")\n",
        "            return float(alpha)\n",
        "\n",
        "    print(\"Line search: max_iter reached, returning last alpha.\")\n",
        "    return float(alpha)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_x = np.array([2.0, 2.0])\n",
        "    problem = rosenbrock()\n",
        "\n",
        "    solution, iters, fmin = steepest_descent(\n",
        "        x=base_x,\n",
        "        problem=problem,\n",
        "        tol=1e-6,\n",
        "        iteration_limit=100000,\n",
        "    )\n",
        "\n",
        "    print(\"\\nFINAL RESULTS:\")\n",
        "    print(f\"Solution: {solution}\")\n",
        "    print(f\"Iterations: {iters}\")\n",
        "    print(f\"Minimum f(x): {fmin}\")\n",
        "\n",
        "\"\"\"\n",
        "--------------------------------------------------------\n",
        "with this approaach line search is only called one at beginning and then with the\n",
        "alpha is used a constant inside steepest descent.\n",
        "\n",
        "need to call alpha inside steepest descent and thne find optimal alpha at\n",
        "every iteration\n",
        "-------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    base_x = np.array([2.0,2.0])\n",
        "    problem = rosenbrock()\n",
        "    direction = -(problem.grad_objective(base_x))\n",
        "    # static step size, later replaced by line search\n",
        "    alpha = linesearch(base_x=base_x,direction=direction)\n",
        "    '''as we know steepest descent function return three values,\n",
        "    so making 3 variables and assigned the returned values to them'''\n",
        "    solution, iters, fmin = steepest_descent(\n",
        "        x=base_x,\n",
        "        problem=problem,\n",
        "        alpha=alpha\n",
        "    )\n",
        "    '''alpha = linesearch(base_x=base_x)\n",
        "    solution = steepest_descent(x=base_x,alpha=alpha) this is wrong,\n",
        "    variable is assigned as per return value fo the function'''\n",
        "    print(\"\\nFINAL RESULTS:\")\n",
        "    print(f\"Solution:{solution}\")\n",
        "    print(f\"Iterations:{iters}\")\n",
        "    print(f\"Minimum f(x):{fmin}\")\n",
        "\"\"\"\n"
      ]
    }
  ]
}